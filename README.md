# Aura: The Contextual AI Assistant

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

**Aura** is a next-generation, open-source assistive technology project. It uses a modern, scalable stack to provide a real-time contextual AI assistant for individuals who are blind or have low vision.

This system is not just a screen reader; it's a real-world copilot that can see, hear, remember, and connect the user with a sighted assistant when needed.

---

### ğŸš€ Key Features

* **Conversational AI:** Human-like, real-time voice conversations powered by **Vapi**.
* **Real-Time Context:** Uses live video, GPS, and sensor data to understand the user's environment.
* **Hybrid Computer Vision:** Leverages **OpenCV** for fast, efficient, on-device (or server-side) pre-processing, object detection, and analysis before sending data to the cloud AI.
* **5-Minute Memory:** A robust video pipeline archives the user's stream to **S3**, allowing them to ask questions about the recent past ("What did I just put down?").
* **Remote Sighted Assistance:** A secure web portal for family or friends to log in, see the user's live feed, and provide direct audio assistance.
* **Scalable by Design:** Built on a robust, serverless stack designed to scale from one user to thousands.

---

### ğŸ› ï¸ Tech Stack & Architecture

This project is a high-performance **[Turborepo](https://turbo.build/repo)** monorepo.

Using a monorepo allows us to manage all our code in a single repository while sharing code, types, and configurations across applications.

* **Monorepo:** **Turborepo**
* **Voice AI:** **Vapi** (for real-time, human-like voice)
* **Core AI:** **OpenAI (GPT-4o)** for multimodal vision and reasoning.
* **Computer Vision:** **OpenCV** (for real-time video processing and analysis)
* **Authentication:** **Clerk** (for both mobile and web)
* **Database:** **Supabase** (Postgres DB, Auth integration, Edge Functions)
* **Web Portal (`apps/web`):** **Next.js** & Tailwind CSS
* **Mobile App (`apps/mobile`):** **React Native (Expo)**
* **Video Ingest Service (`apps/ingest`):** A **Node.js (Fastify/Express)** service to receive the ESP32-CAM stream, process frames with OpenCV, and upload to S3.
* **Real-time Data:** **Redis** (for GPS/sensor Pub/Sub)
* **Video Storage:** **Amazon S3** (for the 5-minute rolling video buffer)
* **Hardware:** **ESP32-CAM** & **ESP8266/LiDAR**

---

### ğŸ“¦ Project Structure

The monorepo contains `apps` (our user-facing applications and services) and `packages` (shared code).

```
/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ ingest/                # Vision ingest & perception pipeline
â”‚       â”œâ”€â”€ package.json
â”‚       â”œâ”€â”€ python/             # Python-based vision workers
â”‚       â”‚   â”œâ”€â”€ webcam_perception.py
â”‚       â”‚   â”œâ”€â”€ vision_worker.py
â”‚       â”‚   â”œâ”€â”€ scene.json
â”‚       â”‚   â””â”€â”€ face_db/
â”‚       â””â”€â”€ src/                # TypeScript ingest + fusion layer
â”‚           â”œâ”€â”€ api/
â”‚           â”‚   â”œâ”€â”€ ingest.ts
â”‚           â”‚   â””â”€â”€ context.ts
â”‚           â”œâ”€â”€ camera/
â”‚           â”‚   â””â”€â”€ esp32.ts
â”‚           â”œâ”€â”€ context/
â”‚           â”‚   â””â”€â”€ scene.ts
â”‚           â”œâ”€â”€ fusion/
â”‚           â”‚   â””â”€â”€ spatial.ts
â”‚           â”œâ”€â”€ sensors/
â”‚           â”‚   â””â”€â”€ lidar.ts
â”‚           â”œâ”€â”€ vision/
â”‚           â”‚   â”œâ”€â”€ yolo.ts
â”‚           â”‚   â”œâ”€â”€ faces.ts
â”‚           â”‚   â””â”€â”€ annotate.ts
â”‚           â””â”€â”€ index.ts
â”‚
â”œâ”€â”€ pnpm-lock.yaml
â”œâ”€â”€ turbo.json
â””â”€â”€ README.md

```
